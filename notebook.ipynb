{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP : Analyse de sentiments dans les critiques de films"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Pierre Fontaine_ \n",
    "#### X 2016 - Master 2 Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "1. Implémenter une manière simple de représenter des données textuelles - Bag of words\n",
    "2. Implémenter un modèle d'apprentissage statistique basique - Bayésien Naïf\n",
    "3. Utiliser ces représentations et ce modèle pour une tâche d'analyse de sentiments\n",
    "4. Implémenter différentes manières d'obtenir des représentations denses des mêmes données\n",
    "5. Utiliser un modèle de régression logistique pour entraîner un classifieur sur ces nouvelles représentations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dépendances nécessaires\n",
    "\n",
    "Pour commencer, on aura besoin des packages suivants:\n",
    "- The Machine Learning API Scikit-learn : http://scikit-learn.org/stable/install.html\n",
    "- The Natural Language Toolkit : http://www.nltk.org/install.html\n",
    "\n",
    "Les deux sont disponibles avec Anaconda: https://anaconda.org/anaconda/nltk et https://anaconda.org/anaconda/scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import re \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger les données\n",
    "\n",
    "On récupère les données textuelles dans la variable *texts*\n",
    "\n",
    "On récupère les labels dans la variable $y$ qui en contient *len(texts)* : $0$ indique que la critique correspondante est négative tandis que $1$ qu'elle est positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 documents\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "# We get the files from the path: ./data/imdb1/neg for negative reviews, and ./data/imdb1/pos for positive reviews\n",
    "filenames_neg = sorted(glob(op.join('.', 'data', 'imdb1', 'neg', '*.txt')))\n",
    "filenames_pos = sorted(glob(op.join('.', 'data', 'imdb1', 'pos', '*.txt')))\n",
    "\n",
    "# Each files contains a review that consists in one line of text: we put this string in two lists, that we concatenate\n",
    "texts_neg = [open(f, encoding=\"utf8\").read() for f in filenames_neg]\n",
    "texts_pos = [open(f, encoding=\"utf8\").read() for f in filenames_pos]\n",
    "texts = texts_neg + texts_pos\n",
    "\n",
    "# The first half of the elements of the list are string of negative reviews, and the second half positive ones\n",
    "# We create the labels, as an array of [1,len(texts)], filled with 1, and change the first half to 0\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "print(\"%d documents\" % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de documents: 2500\n"
     ]
    }
   ],
   "source": [
    "# This number of documents may be high for most computers: we can select a fraction of them (here, one in k)\n",
    "# Use an even number to keep the same number of positive and negative reviews\n",
    "# Attention à bien choisir le paramètre k !\n",
    "\n",
    "k = 10\n",
    "texts_reduced = texts[0::k]\n",
    "y_reduced = y[0::k]\n",
    "\n",
    "print('Nombre de documents:', len(texts_reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayésien Naïf \n",
    "\n",
    "## Idée principale\n",
    "\n",
    "On dispose d'une critique étant en fait une liste de mots $s = (w_1, ..., w_N)$, et l'on cherche à trouver la classe associée $c$ - qui dans notre cas, peut-être $c = 0$ ou $c = 1$. L'objectif est donc de trouver pour chaque critique $s$ la classe $\\hat{c}$ maximisant la probabilité conditionelle **$P(c|s)$** : \n",
    "\n",
    "$$\\hat{c} = \\underset{c}{\\mathrm{argmax}}\\, P(c|s) = \\underset{c}{\\mathrm{argmax}}\\,\\frac{P(s|c)P(c)}{P(s)}$$\n",
    "\n",
    "**Hypothèse : P(s) est constante pour chaque classe** :\n",
    "\n",
    "$$\\hat{c} = \\underset{c}{\\mathrm{argmax}}\\,\\frac{P(s|c)P(c)}{P(s)} = \\underset{c}{\\mathrm{argmax}}\\,P(s|c)P(c)$$\n",
    "\n",
    "**Hypothèse naïve : les différentes variables (mots) d'une critique sont indépendantes entre elles** : \n",
    "\n",
    "$$P(s|c) = P(w_1, ..., w_N|c)=\\Pi_{i=1..N}P(w_i|c)$$\n",
    "\n",
    "On va donc pouvoir se servir des critiques annotées à notre disposition pour **estimer les probabilités $P(w|c)$ pour chaque mot $w$ étant donné les deux classes $c$**. Ces critiques vont nous permettre d'apprendre à évaluer la \"compatibilité\" entre les mots et classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vue générale\n",
    "\n",
    "### Entraînement: Estimer les probabilités\n",
    "\n",
    "Pour chaque mot $w$ du vocabulaire $V$, $P(w|c)$ est le nombre d'occurences de $w$ dans une critique ayant pour classe $c$, divisé par le nombre total d'occurences dans $c$. Si on note $T(w,c)$ ce nombre d'occurences, on obtient:\n",
    "\n",
    "$$P(w|c) = \\text{Fréquence de }w\\text{ dans }c = \\frac{T(w,c)}{\\sum_{w' \\in V} T(w',c)}$$\n",
    "\n",
    "### Test: Calcul des scores\n",
    "\n",
    "Pour faciliter les calculs et éviter les erreurs d'*underflow* et d'approximation, on utilise le \"log-sum trick\", et on passe l'équation en log-probabilités : \n",
    "\n",
    "$$\\hat{c} =  \\underset{c}{\\mathrm{argmax}}\\, P(c|s) = \\underset{c}{\\mathrm{argmax}}\\, \\left[ \\mathrm{log}(P(c)) + \\sum_{i=1..N}log(P(w_i|c)) \\right]$$\n",
    "\n",
    "### Laplace smoothing (Lissage)\n",
    "\n",
    "Un mot qui n'apparaît pas dans un document a une probabilité nulle: cela va poser problème avec le logarithme. On garde donc une toute petite partie de la masse de probabilité qu'on redistribue avec le *Laplace smoothing*: \n",
    "\n",
    "$$P(w|c) = \\frac{T(w,c) + 1}{\\sum_{w' \\in V} T(w',c) + 1}$$\n",
    "\n",
    "Il existe d'autre méthodes de lissage, en général adaptées à d'autres applications plus complexes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentation adaptée des documents\n",
    "\n",
    "Notre modèle statistique, comme la plupart des modèles appliqués aux données textuelles, utilise les comptes d'occurences de mots dans un document. Ainsi, une manière très pratique de représenter un document est d'utiliser un vecteur \"Bag-of-Words\" (BoW), contenant les comptes de chaque mot (indifférement de leur ordre d'apparition) dans le document. \n",
    "\n",
    "Si on considère l'ensemble de tous les mots apparaissant dans nos $T$ documents d'apprentissage, que l'on note $V$ (Vocabulaire), on peut créer **un index**, qui est une bijection associant à chaque mot $w$ un entier, qui sera sa position dans $V$. \n",
    "\n",
    "Ainsi, pour un document extrait d'un ensemble de documents contenant $|V|$ mots différents, une représentation BoW sera un vecteur de taille $|V|$, dont la valeur à l'indice d'un mot $w$ sera son nombre d'occurences dans le document. \n",
    "\n",
    "On peut utiliser la classe **CountVectorizer** de scikit-learn pour mieux comprendre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['avenue', 'boulevard', 'city', 'down', 'ran', 'the', 'walk', 'walked']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 2, 0, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 1, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 1, 1, 0],\n",
       "       [1, 0, 0, 1, 0, 2, 1, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['I walked down down the boulevard',\n",
    "          'I walked down the avenue',\n",
    "          'I ran down the boulevard',\n",
    "          'I walk down the city',\n",
    "          'I walk down the the avenue']\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "Bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "Bow.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche d'abord la liste contenant les mots ordonnés selon leur indice (On note que les mots de 2 caractères ou moins ne sont pas pris en compte)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Détail: entraînement\n",
    "\n",
    "L'idée est d'extraire le nombre d'occurences $T(w,c)$ de chaque mot $w$ pour chaque classe $c$, ce qui permettra de calculer la matrice de probabilités conditionelles $\\pmb{P}$ telle que: $$\\pmb{P}_{w,c} = P(w|c)$$\n",
    "\n",
    "Notons que le nombre d'occurences $T(w,c)$ peut être obtenu facilement à partir des représentations BoW de l'ensemble des documents.\n",
    "\n",
    "### Procédure:\n",
    "<img src=\"algo_train.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "## Détail: test\n",
    "\n",
    "Nous connaissons maintenant les probabilités conditionelles données par la matrice $\\pmb{P}$. \n",
    "Il faut maintenant obtenir $P(s|c)$ pour le document courant. Cette quantité s'obtient à l'aide d'un calcul simple impliquant la représentation BoW du document et $\\pmb{P}$.\n",
    "\n",
    "### Procédure:\n",
    "<img src=\"algo_test.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing du texte: obtenir les représentations BoW\n",
    "\n",
    "D'abord, il faut transformer les critiques sous forme de strings en une liste de mots. La tactique la plus simple consiste à diviser le string suivant les espaces, avec la commande:\n",
    "```text.split()```\n",
    "\n",
    "Mais il faut aussi faire attention à enlever les caractères particuliers qui pourraient ne pas avoir été nettoyés (comme les balises HTML si on a obtenu les données à partir de pages web). Puisque l'on va compter les mots, il faudra construire une liste des mots apparaissant dans nos données. Dans notre cas, on aimerait réduire cette liste et l'uniformiser (ignorer les majuscules, la ponctuation, et les mots les plus courts). \n",
    "On va donc utiliser une fonction adaptée à nos besoins - mais c'est un travail qu'il n'est en général pas nécessaire de faire, puisqu'il existe de nombreux outils déjà adaptés à la plupart des cas de figures. \n",
    "Pour le nettoyage du texte, il existe de nombreux scripts, basés sur différents outils (expressions régulières, par exemple) qui permettent de préparer des données. La division du texte en mots et la gestion de la ponctuation est gérée lors d'une étape appellée *tokenization*; si besoin, un package python comme le NLTK contient de nombreux *tokenizers* différents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We might want to clean the file with various strategies:\n",
    "def clean_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    Cleaning a document with:\n",
    "        - Lowercase        \n",
    "        - Removing numbers with regular expressions\n",
    "        - Removing punctuation with regular expressions\n",
    "        - Removing other artifacts\n",
    "    And separate the document into words by simply splitting at spaces\n",
    "    Params:\n",
    "        text (string): a sentence or a document\n",
    "    Returns:\n",
    "        tokens (list of strings): the list of tokens (word units) forming the document\n",
    "    \"\"\"        \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "    # Remove punctuation\n",
    "    REMOVE_PUNCT = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "    text = REMOVE_PUNCT.sub(\"\", text)\n",
    "    # Remove small words (1 and statistique2 characters)\n",
    "    text = re.sub(r\"\\b\\w{1,2}\\b\", \"\", text)\n",
    "    # Remove HTML artifacts specific to the corpus we're going to work with\n",
    "    REPLACE_HTML = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    text = REPLACE_HTML.sub(\" \", text)\n",
    "    \n",
    "    tokens = text.split()        \n",
    "    return tokens\n",
    "\n",
    "# Or we might want to use an already-implemented tool. The NLTK package has a lot of very useful text processing tools, among them various tokenizers\n",
    "# Careful, NLTK was the first well-documented NLP package, but it might be outdated for some uses. Check the documentation !\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "corpus_raw = \"I walked down down the boulevard. I walked down the avenue. I ran down the boulevard. I walk down the city. I walk down the the avenue.\"\n",
    "#print(clean_and_tokenize(corpus_raw))\n",
    "#print(word_tokenize(corpus_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction **à compléter**. Elle prend en entrée une liste de document (chacun sous la forme d'un string) et renvoie, comme dans l'exemple utilisant ```CountVectorizer```:\n",
    "- Un vocabulaire qui associe à chaque mot rencontré un index\n",
    "- Une matrice, dont les lignes représentent les documents et les colonnes les mots indexés par le vocabulaire. En position $(i,j)$, on devra avoir le nombre d'occurence du mot $j$ dans le document $i$.\n",
    "\n",
    "Le vocabulaire, qui était sous la forme d'une *liste* dans l'exemple précédent, pourra être renvoyé sous forme de *dictionnaire* dont les clés sont les mots et les valeurs les indices. Puisque le vocabulaire recense les mots du corpus sans se soucier de leur nombre d'occurences, on pourra le constituer à l'aide d'un ensemble (```set``` en python). \n",
    "On pourra bien sur utiliser la fonction ```clean_and_tokenize``` pour transformer les strings en liste de mots. \n",
    "##### Quelques pointeurs pour les débutants en Python : \n",
    "\n",
    "- ```my_list.append(value)``` : put the variable ```value``` at the end of the list ```my_list```\n",
    "\n",
    "-  ```words = set()``` : create a set, which is a list of unique values \n",
    "\n",
    "- ```words.union(my_list)``` : extend the set ```words```\n",
    "\n",
    "- ```dict(zip(keys, values)))``` : create a dictionnary\n",
    "\n",
    "- ```for k, text in enumerate(texts)``` : syntax for a loop with the index, ```texts``` begin a list (of texts !)\n",
    "\n",
    "- ```len(my-list)``` : length of the list ```my_list```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(texts):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## On parcourt tous les textes du corpus 'texts' et on obtient l'ensemble des mots 'words' en invoquant la\n",
    "    ## fonction 'clean_and_tokenize'\n",
    "    \n",
    "    words = set()\n",
    "    for text in texts:\n",
    "        words = words.union(set(clean_and_tokenize(text)))\n",
    "    \n",
    "    ## Le dictionnaire 'vocabulary' a pour clefs les mots 'words' et valeurs des indices entiers\n",
    "    \n",
    "    n_features = len(words)\n",
    "    values = set(range(n_features))\n",
    "    vocabulary = dict(zip(words, values))\n",
    "    \n",
    "    counts = np.zeros(shape=(len(texts), n_features), dtype=int)\n",
    "    # On parcourt les textes du corpus\n",
    "    for k, text in enumerate(texts):\n",
    "        # On incrémente la matrice 'counts' pour l'indice du vocabulaire correspondant au mot 'word'\n",
    "        for word in clean_and_tokenize(text):\n",
    "            counts[k][vocabulary[word]] += 1\n",
    "            \n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'walk': 0, 'walked': 1, 'the': 2, 'avenue': 3, 'city': 4, 'ran': 5, 'down': 6, 'boulevard': 7}\n",
      "[[0 1 1 0 0 0 2 1]\n",
      " [0 1 1 1 0 0 1 0]\n",
      " [0 0 1 0 0 1 1 1]\n",
      " [1 0 1 0 1 0 1 0]\n",
      " [1 0 2 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "Voc, X = count_words(corpus)\n",
    "print(Voc)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayésien Naïf \n",
    "\n",
    "Classe vide : fonctions **à compléter** : \n",
    "\n",
    "```python\n",
    "def fit(self, X, y)\n",
    "``` \n",
    "**Entraînement** : va apprendre un modèle statistique basés sur les représentations $X$ correspondant aux labels $y$.\n",
    "$X$ représente donc ici des représentations obtenues en sortie de count_words. On complète la fonction à l'aide de la procédure détaillée plus haut. Si il est possible de la suivre à la lettre, les représentations que l'on utilise nous permettre d'être bien plus efficace et d'éviter d'utiliser des boucles !\n",
    "\n",
    "\n",
    "Note: le lissage, effectué à la ligne $10$, ne se fait pas nécessairement avec un $1$, mais peut se faire avec une valeur positive $\\alpha$, qu'on peut implémenter comme argument de la classe ```NB```.\n",
    "\n",
    "```python\n",
    "def predict(self, X)\n",
    "```\n",
    "**Testing** : va renvoyer les labels prédits par le modèle pour d'autres représentations $X$.\n",
    "\n",
    "\n",
    "Pour faciliter la procédure, on prendra la moitié de la matrice $X$ obtenue plus haut pour entraîner le modèle, et l'autre moitié pour l'évaluer. **Important**: cette façon de procéder n'est pas réaliste: en général, on ne dispose que des données d'entraînement au moment de créer le vocabulaire et d'entraîner le modèle. Ainsi, il est possible que les données d'évaluation contiennent des mots *inconnus*. C'est quelque chose qu'on peut traiter facilement en dédiant un indice à tous les mots rencontrés qui ne sont pas contenus dans le vocabulaire - mais il existe de nombreuses méthodes plus complexes pour réussir à utiliser à bon escient ces mots que le modèle n'a pas rencontré à l'entraînement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quelques pointeurs pour les débutants en Python : \n",
    "\n",
    "Utiliser l'API Numpy pour travailler avec des tenseurs\n",
    "\n",
    "\n",
    "- ```X.shape``` : for a ```numpy.array```, return the dimension of the tensor\n",
    "\n",
    "- ```np.zeros((dim_1, dim_2,...))``` : create a tensor filled with zeros\n",
    "\n",
    "- ```np.sum(X, axis = n)``` : sum the tensor over the axis n\n",
    "\n",
    "- ```np.mean(X, axis = n)```\n",
    "\n",
    "- ```np.argmax(X, axis = n)```\n",
    "\n",
    "- ```np.log(X)```\n",
    "\n",
    "- ```np.dot(X_1, X_1)``` : Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    # Les arguments de classe permettent l'héritage de classes de sklearn\n",
    "    def __init__(self, alpha=1.0):\n",
    "        # alpha est un paramètre pour le lissage: il correspond à la valeur ligne 10 de l'algorithme d'entraînement\n",
    "        # Dans l'algorithme d'entraînement, et comme valeur par défaut, on utilise alpha = 1\n",
    "        self.alpha = alpha     \n",
    "        \n",
    "    ## On applique l'algorithme TRAINMULTINOMIALNB pour deux classes '0' et '1', ce qui correspond finalement à\n",
    "    ## TRAINBINOMIALNB\n",
    "    \n",
    "    ## Remarque : on a choisi de considérer directement les logarithmes des priors et des probabilités\n",
    "    ## conditionnelles dans la fonction 'fit'\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Taille du vocabulaire\n",
    "        v = X.shape[1]\n",
    "        # Taille du corpus\n",
    "        n = X.shape[0]\n",
    "        # Logarithmes des probabilités conditionnelles\n",
    "        self.log_cond_prob = np.zeros((v, 2))\n",
    "        # Logarithmes des priors pour les deux classes\n",
    "        self.log_prior = [0, 0]\n",
    "        # Prior de classe c = card(X de classe c) / card(X)\n",
    "        self.log_prior[0], self.log_prior[1] = np.log(len(X[y==0])/n), np.log(len(X[y==1])/n)\n",
    "        # Compteur des tokens selon le vocabulaire, pour chaque classe\n",
    "        compt = np.zeros((v, 2))\n",
    "        # On parcourt le vocabualire\n",
    "        for it in range(v):\n",
    "            # Initialisation à alpha\n",
    "            compt[it][0], compt[it][1] = self.alpha, self.alpha\n",
    "            # On parcourt la classe '0'\n",
    "            for x_0 in X[y==0]:\n",
    "                compt[it][0] += x_0[it]\n",
    "            # On parcourt la classe '1'\n",
    "            for x_1 in X[y==1]:\n",
    "                compt[it][1] += x_1[it]\n",
    "        # On somme les compteurs dans 'acc'\n",
    "        acc = np.sum(compt, axis=0)\n",
    "        acc_0, acc_1 = acc[0], acc[1]\n",
    "        # On calcule les probabilités conditionnelles selon la formule de l'algorithme\n",
    "        for it in range(v):\n",
    "            self.log_cond_prob[it][0] = np.log(compt[it][0] / acc_0)\n",
    "            self.log_cond_prob[it][1] = np.log(compt[it][1] / acc_1)\n",
    "        return self\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        # On parcourt X\n",
    "        for x in X:\n",
    "            # On extrait les mots du texte 'x'\n",
    "            words = []\n",
    "            for it, xx in enumerate(x):\n",
    "                if xx!=0:\n",
    "                    words += xx*[it]\n",
    "            score = np.zeros((2))\n",
    "            # On incrémente les scores pour les deux classes : '0' et '1'\n",
    "            score[0], score[1] = self.log_prior[0], self.log_prior[1]\n",
    "            for word in words:\n",
    "                score[0] += self.log_cond_prob[word][0]\n",
    "                score[1] += self.log_cond_prob[word][1]\n",
    "            # Le résultat privilégié est celui maximisant le score\n",
    "            result.append(np.argmax(score))\n",
    "        return result\n",
    "                    \n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expérimentation\n",
    "\n",
    "On utilise la moitié des données pour l'entraînement, l'autre pour tester le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc, X = count_words(texts_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.796\n"
     ]
    }
   ],
   "source": [
    "nb = NB()\n",
    "nb.fit(X[::2], y_reduced[::2])\n",
    "print(nb.score(X[1::2], y_reduced[1::2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation \n",
    "\n",
    "Avec la fonction *cross_val_score* de scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de classification: 0.7804 (std 0.022249494376277386)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(nb, X, y_reduced, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluer les performances: \n",
    "\n",
    "**Quelles sont les points forts et les points faibles de ce système ? Comment y remédier ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons les résultats suivants pour notre implmentation du Classifieur Bayésien Naïf en fonction de `k` :\n",
    "\n",
    "|`k`| Score| Score de validation croisée|\n",
    "|:-:|:----:|:--------------------------:|\n",
    "| 10| 0.796|           0.7804 (std 0.02)|\n",
    "| 25| 0.778|            0.767 (std 0.02)|\n",
    "\n",
    "Nous verrons au cours de ce TP s'il est possible d'en obtenir de meilleurs. Ils semblent déjà prometteurs car la méthode de classification bayésienne est relativement simple (naïve)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Points forts :* \n",
    "* Le Classifieur Bayésien Naïf a une bonne capacité de classifciation car il repose sur une théorie mathématique efficace quoique naïve. En outre, les résultats obtenus par le classifieur s'expliquent simplement en terme de probabilités.\n",
    "* Il est facile à implémenter et considéré d'éxécution rapide.\n",
    "* On peut facilement l'élargir à une classification multicatégorielle (plus de deux classes) si l'on souhaite considérer une autre classification que la classification binaire.\n",
    "* Il est relativement insensible aux données absurdes ou erronées, il est donc robuste.\n",
    "\n",
    "\n",
    "#### *Point faible :*\n",
    "* Le Classifieur Bayésien Naïf suppose l'indépendance des données : si différents textes sont corrélés, les résultats de la théorie mathématique ne sont plus valides. Cette condition est presque toujours violée dans les cas rééls.\n",
    "* Il est très sensible à la forme des données d'entrée.\n",
    "\n",
    "#### *Solution pour y remédier :*\n",
    "\n",
    "* Même si les données ne sont pas indépendantes, on observe des résultats satisfaisants en pratique. On peut cependant songer à des classifieurs plus complexes, qui prennent en compte les cas de dépendance. Leur implémentation est cependant plus difficile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pour aller plus loin: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Améliorer les représentations\n",
    "\n",
    "On utilise la function \n",
    "```python\n",
    "CountVectorizer\n",
    "``` \n",
    "de scikit-learn pour constituer notre corpus. Elle nous permettra d'améliorer facilement nos représentations BoW.\n",
    "\n",
    "#### Tf-idf:\n",
    "\n",
    "Il s'agit du produit de la fréquence du terme (TF) et de sa fréquence inverse dans les documents (IDF).\n",
    "Cette méthode est habituellement utilisée pour extraire l'importance d'un terme $i$ dans un document $j$ relativement au reste du corpus, à partir d'une matrice d'occurences $mots \\times documents$. Ainsi, pour une matrice $\\mathbf{T}$ de $|V|$ termes et $D$ documents:\n",
    "$$\\text{TF}(T, w, d) = \\frac{T_{w,d}}{\\sum_{w'=1}^{|V|} T_{w',d}} $$\n",
    "\n",
    "$$\\text{IDF}(T, w) = \\log\\left(\\frac{D}{|\\{d : T_{w,d} > 0\\}|}\\right)$$\n",
    "\n",
    "$$\\text{TF-IDF}(T, w, d) = \\text{TF}(X, w, d) \\cdot \\text{IDF}(T, w)$$\n",
    "\n",
    "On peut l'adapter à notre cas en considérant que le contexte du deuxième mot est le document. Cependant, TF-IDF est généralement plus adaptée aux matrices peu denses, puisque cette mesure pénalisera les termes qui apparaissent dans une grande partie des documents. \n",
    "    \n",
    "#### Ne pas prendre en compte les mots trop fréquents:\n",
    "\n",
    "On peut utiliser l'option \n",
    "```python\n",
    "max_df=1.0\n",
    "```\n",
    "pour modifier la quantité de mots pris en compte. \n",
    "\n",
    "#### Essayer différentes granularités:\n",
    "\n",
    "Plutôt que de simplement compter les mots, on peut compter les séquences de mots - de taille limitée, bien sur. \n",
    "On appelle une séquence de $n$ mots un $n$-gram: essayons d'utiliser les 2 et 3-grams (bi- et trigrams).\n",
    "On peut aussi tenter d'utiliser les séquences de caractères à la place de séquences de mots.\n",
    "\n",
    "On s'intéressera aux options \n",
    "```python\n",
    "analyzer='word'\n",
    "```\n",
    "et \n",
    "```python\n",
    "ngram_range=(1, 2)\n",
    "```\n",
    "que l'on changera pour modifier la granularité. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification score: 0.781 (std 0.021)\n",
      "Classification score tf-idf: 0.783 (std 0.024)\n",
      "Classification score sans mots fréquents: 0.785 (std 0.019)\n",
      "Classification score bigram: 0.805 (std 0.017)\n",
      "Classification score trigram: 0.816 (std 0.016)\n",
      "Classification score trigram TF-IDF: 0.812 (std 0.022)\n",
      "Classification score quadrigram: 0.816 (std 0.019)\n",
      "Classification score pentagram: 0.815 (std 0.02)\n",
      "Classification score char and bigram: 0.682 (std 0.03)\n",
      "Classification score char and trigram: 0.722 (std 0.033)\n",
      "Classification score char and quadrigram: 0.751 (std 0.027)\n"
     ]
    }
   ],
   "source": [
    "## On peut définir une pipeline que l'on modifiera pour expérimenter.\n",
    "\n",
    "## Base\n",
    "\n",
    "pipeline_base = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', stop_words=None)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "scores = cross_val_score(pipeline_base, texts_reduced, y_reduced, cv=5)\n",
    "print(\"Classification score: %s (std %s)\" % (round(np.mean(scores), 3), round(np.std(scores), 3)))\n",
    "\n",
    "## TF-IDF\n",
    "\n",
    "pipeline_tf_idf = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', stop_words=None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "scores = cross_val_score(pipeline_tf_idf, texts_reduced, y_reduced, cv=5)\n",
    "print(\"Classification score tf-idf: %s (std %s)\" % (round(np.mean(scores), 3), round(np.std(scores), 3)))\n",
    "\n",
    "## Max DF (choisir une fraction entre 0 et 1 afin de conserver la proportion de mots)\n",
    "\n",
    "pipeline_maxdf = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', stop_words=None, max_df=0.25)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "scores = cross_val_score(pipeline_maxdf, texts_reduced, y_reduced, cv=5)\n",
    "print(\"Classification score sans mots fréquents: %s (std %s)\" % (round(np.mean(scores), 3), round(np.std(scores), 3)))\n",
    "\n",
    "## Bigram Words\n",
    "\n",
    "pipeline_bigram = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', stop_words=None, ngram_range=(1, 2))),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "scores = cross_val_score(pipeline_bigram, texts_reduced, y_reduced, cv=5)\n",
    "print(\"Classification score bigram: %s (std %s)\" % (round(np.mean(scores), 3), round(np.std(scores), 3)))\n",
    "\n",
    "## Trigram Words\n",
    "\n",
    "pipeline_trigram = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', stop_words=None, ngram_range=(1, 3))),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "scores = cross_val_score(pipeline_trigram, texts_reduced, y_reduced, cv=5)\n",
    "print(\"Classification score trigram: %s (std %s)\" % (round(np.mean(scores), 3), round(np.std(scores), 3)))\n",
    "\n",
    "## Trigram Words TF-IDF\n",
    "\n",
    "pipeline_trigram_tfidf = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', stop_words=None, ngram_range=(1, 3))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "scores = cross_val_score(pipeline_trigram_tfidf, texts_reduced, y_reduced, cv=5)\n",
    "print(\"Classification score trigram TF-IDF: %s (std %s)\" % (round(np.mean(scores), 3), round(np.std(scores), 3)))\n",
    "\n",
    "## Quadrigram Words\n",
    "\n",
    "pipeline_quadrigram = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', stop_words=None, ngram_range=(1, 4))),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "scores = cross_val_score(pipeline_quadrigram, texts_reduced, y_reduced, cv=5)\n",
    "print(\"Classification score quadrigram: %s (std %s)\" % (round(np.mean(scores), 3), round(np.std(scores), 3)))\n",
    "\n",
    "## Pentagram Words\n",
    "\n",
    "pipeline_pentagram = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', stop_words=None, ngram_range=(1, 6))),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "scores = cross_val_score(pipeline_pentagram, texts_reduced, y_reduced, cv=5)\n",
    "print(\"Classification score pentagram: %s (std %s)\" % (round(np.mean(scores), 3), round(np.std(scores), 3)))\n",
    "\n",
    "## Bigram Char\n",
    "\n",
    "pipeline_char = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='char', stop_words=None, ngram_range=(1, 2))),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "scores = cross_val_score(pipeline_char, texts_reduced, y_reduced, cv=5)\n",
    "print(\"Classification score char and bigram: %s (std %s)\" % (round(np.mean(scores), 3), round(np.std(scores), 3)))\n",
    "\n",
    "## Trigram Char\n",
    "\n",
    "pipeline_char_trigram = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='char', stop_words=None, ngram_range=(1, 3))),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "scores = cross_val_score(pipeline_char_trigram, texts_reduced, y_reduced, cv=5)\n",
    "print(\"Classification score char and trigram: %s (std %s)\" % (round(np.mean(scores), 3), round(np.std(scores), 3)))\n",
    "\n",
    "## Quadrigram Char\n",
    "\n",
    "pipeline_char_quadri = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='char', stop_words=None, ngram_range=(1, 4))),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "scores = cross_val_score(pipeline_char_quadri, texts_reduced, y_reduced, cv=5)\n",
    "print(\"Classification score char and quadrigram: %s (std %s)\" % (round(np.mean(scores), 3), round(np.std(scores), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats obtenus avec les différentes _pipelines_ sont, pour `k=10` :\n",
    "\n",
    "|   Pipeline|Score de validation croisée|\n",
    "|:--------:|:-------------------------:|\n",
    "|   Basique| 0.781 (std 0.021)|\n",
    "|TF-IDF| 0.783 (std 0.024) |\n",
    "|Sans mots fréquents| 0.785 (std 0.019)|\n",
    "|Bigram|0.805 (std 0.017)|\n",
    "|Trigram|0.816 (std 0.016)|\n",
    "|Trigram & TF-IDF|0.812 (std 0.022)|\n",
    "|Quadrigram|0.816 (std 0.019)|\n",
    "|Pentagram|0.815 (std 0.02)|\n",
    "|Char & Bigram|0.682 (std 0.03)|\n",
    "|Char & Trigram| 0.722 (std 0.033)|\n",
    "|Char & Quadrigram| 0.751 (std 0.027)|\n",
    "\n",
    "\n",
    "\n",
    "- On remarque que les classifications sur des mots avec des $n$-_grams_ ont de bons résultats, mais plus $n$ est grand, plus le calcul est coûteux (le temps de calcul reste cependant acceptable dans notre cas). Le _trigram_ est le cas préférable.\n",
    "\n",
    "- L'utilisation du transformeur TF-IDF donne un résultat légérement moins performant que celui de la classification basique, ou bien même si on compose aussi avec une approche $n$-_gram_.\n",
    "\n",
    "- Les  résultats sur les séquences de caractères sont moins performants, ce qui semble cohérent puisque l'analyse sur les mots semble davantage cohérente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Toolkit (NLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming \n",
    "\n",
    "Permet de revenir à la racine d'un mot: on peut ainsi grouper différents mots autour de la même racine, ce qui facilite la généralisation. Utiliser:\n",
    "```python\n",
    "from nltk import SnowballStemmer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple d'utilisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word : singers ; stemmed : singer\n",
      "word : cat ; stemmed : cat\n",
      "word : generalization ; stemmed : general\n",
      "word : philosophy ; stemmed : philosophi\n",
      "word : psychology ; stemmed : psycholog\n",
      "word : philosopher ; stemmed : philosoph\n"
     ]
    }
   ],
   "source": [
    "words = ['singers', 'cat', 'generalization', 'philosophy', 'psychology', 'philosopher']\n",
    "for word in words:\n",
    "    print(\"word :\", word, \"; stemmed :\", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation des données:\n",
    "\n",
    "Classe vide : function **à compléter** \n",
    "```python\n",
    "def stem(X)\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(X): \n",
    "    \n",
    "    ## On parcourt les textes de 'X' et on applique la fonction 'stemmer.stem' à chaque mot du texte 'text' auquel\n",
    "    ## on a auparavant appliqué la fonction 'clean_and_tokenize'\n",
    "    \n",
    "    X_stem = []\n",
    "    for text in X:\n",
    "        text_stem = \"\"\n",
    "        for word in clean_and_tokenize(text):\n",
    "            # On sépare les mots par un espace, on ajoute le mot de 'stem(word)' au texte 'text_stem'\n",
    "            text_stem += \" \" + stemmer.stem(word) \n",
    "        X_stem.append(text_stem)\n",
    "    return X_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de classification: 0.7708 (std 0.016473008225579216)\n"
     ]
    }
   ],
   "source": [
    "texts_stemmed = stem(texts_reduced)\n",
    "voc, X = count_words(texts_stemmed)\n",
    "nb = NB()\n",
    "\n",
    "scores = cross_val_score(nb, X, y_reduced, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate une baisse du score : celui-ci est de $0.771$ alors qu'on avait précédemment un score de $0.780$ avec `nb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie du discours\n",
    "\n",
    "Pour généraliser, on peut aussi utiliser les parties du discours (Part of Speech, POS) des mots, ce qui nous permettra  de filtrer l'information qui n'est potentiellement pas utile au modèle. On va récupérer les POS des mots à l'aide des fonctions:\n",
    "```python\n",
    "from nltk import pos_tag, word_tokenize\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple d'utilisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#print(pos_tag(word_tokenize(('I am Sam'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Détails des significations des tags POS: https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Transformation des données:\n",
    "\n",
    "Classe vide : fonction **à compléter** \n",
    "```python\n",
    "def pos_tag_filter(X, good_tags=['NN', 'VB', 'ADJ', 'RB'])\n",
    "``` \n",
    "\n",
    "Ne garder que les noms, adverbes, verbes et adjectifs pour notre modèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_filter(X, good_tags=['NN', 'VB', 'ADJ', 'RB']):\n",
    "    X_pos = []\n",
    "    \n",
    "    ## On parcourt les éléments de 'X', on retient uniquement les éléments dont le 'ptag' est \n",
    "    ## dans la liste 'good_tags'\n",
    "    \n",
    "    for text in X:\n",
    "        text_pos = \"\"\n",
    "        for ptag in pos_tag(word_tokenize(text)):\n",
    "            # Test d'appartenance\n",
    "            if ptag[1] in good_tags:\n",
    "                # On sépare les mots par un espace, on ajoute le mot de 'ptag' au texte 'text_pos'\n",
    "                text_pos += \" \" + ptag[0]\n",
    "        X_pos.append(text_pos)\n",
    "        \n",
    "    return X_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de classification: 0.7464000000000001 (std 0.014164744967700631)\n"
     ]
    }
   ],
   "source": [
    "texts_POS = pos_tag_filter(texts_reduced)\n",
    "voc, X = count_words(texts_POS)\n",
    "nb = NB()\n",
    "\n",
    "scores = cross_val_score(nb, X, y_reduced, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate une baisse du score : celui-ci est de $0.746$ alors qu'on avait précédemment un score de $0.780$ et de $0.771$ pour le texte après _stemmatization_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé des résultats avec le Classifieur Bayésien Naïf\n",
    "\n",
    "#### Résultats obtenus pour `k=10` (corpus de $2500$ textes)\n",
    "\n",
    "|Méthode |Score de validation croisée|\n",
    "|:------:|:-------------------------:|\n",
    "| Basique|          0.7804 (std 0.02)|\n",
    "|  TF-IDF|          0.783 (std 0.024)|                \n",
    "| Bi-gram|          0.805 (std 0.017)|\n",
    "|Tri-gram|          0.816 (std 0.016)|\n",
    "| Stemmed|         0.7708 (std 0.016)|\n",
    "| POS-tag|         0.7464 (std 0.014)|\n",
    "\n",
    "\n",
    "#### Résultats obtenus pour `k=25` (corpus de $1000$ textes)\n",
    "\n",
    "|Méthode |Score de validation croisée|\n",
    "|:------:|:-------------------------:|\n",
    "| Basique|           0.767 (std 0.02)|\n",
    "|  TF-IDF|           0.749 (std 0.03)|\n",
    "| Bi-gram|           0.797 (std 0.02)|\n",
    "|Tri-gram|           0.801 (std 0.02)|\n",
    "| Stemmed|           0.759 (std 0.02)|\n",
    "| POS-tag|           0.713 (std 0.01)|     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion sur ces résultats\n",
    "\n",
    "- Le _stemming_ et le _POS-tag filtering_ donnent des résultats moins performants que la méthode basique en terme de scores.\n",
    "- Les méthodes utilisant le _bi-gram_ ou le _tri-gram_ sont les plus prometteuses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation d'un classifieur plus complexe ?\n",
    "\n",
    "On peut utiliser les implémentations scikit-learn de classifieurs moins naïfs, comme la régression logistique ou les SVM. Quel en est l'inconvénient principal (imaginons que, plutôt qu'un modèle linéaire, on choisisse d'utiliser un réseau de neurones à plusieurs couches cachées ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification score Régression Logistique: 0.8084 (std 0.007310266752998798)\n",
      "Classification score SVM: 0.7988000000000001 (std 0.008352245207128448)\n"
     ]
    }
   ],
   "source": [
    "pipeline_logistic = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', LogisticRegression(solver='lbfgs', max_iter = 10000)),\n",
    "])\n",
    "\n",
    "pipeline_svc = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', LinearSVC(max_iter = 10000)),\n",
    "])\n",
    "\n",
    "scores = cross_val_score(pipeline_logistic, texts_reduced, y_reduced, cv=5)\n",
    "print(\"Classification score Régression Logistique: %s (std %s)\" % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "scores = cross_val_score(pipeline_svc, texts_reduced, y_reduced, cv=5)\n",
    "print(\"Classification score SVM: %s (std %s)\" % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats obtenus (`k=10`) sont :\n",
    "\n",
    "| Classifieur | Score de validation croisée |\n",
    "|:--------------:|:---------:|\n",
    "|Régression Logistique| 0.8084 (std 0.007)|\n",
    "|SVM| 0.7988 (std 0.008)|\n",
    "\n",
    "\n",
    "* Les résultats obtenus sont du même ordre voire inférieurs à ceux que nous avions ci-dessus avec le Classifieur Bayésien Naïf, et l'utilisation d'une granularité de $3$. Ils n'offrent donc pas d'amélioration notable.\n",
    "\n",
    "* Pour un jeu de données trop grand, on a un risque d'_overfitting_ pour non seulement la Régression Logistique, mais aussi les SVM.\n",
    "\n",
    "* Nous pouvons également regretter l'absence de modèle probabilistique compréhensible comme c'était le cas avec le Classifieur Bayésien Naïf.\n",
    "\n",
    "* Bien que ce soit le plus simple, le Classifieur Bayésien Naïf doit être préféré dans notre cas aux classifieurs plus complexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Représentations denses\n",
    "\n",
    "##  Word Embeddings : Représentations distribuées via l'hypothèse distributionelle\n",
    "\n",
    "**But**: On va chercher à obtenir des représentations denses (comme vecteurs de nombres réels) de mots (et éventuellement de phrases). Ces représentations ont vocation à être distribuées: ce sont des représentations non-locales. On représente un objet comme une combinaison de *features*, par opposition à l'attribution d'un symbole dédié: voir le travail fondateur d'entre autres, Geoffrey Hinton, sur le sujet: [Distributed Representations](https://web.stanford.edu/~jlmcc/papers/PDP/Chapter3.pdf).\n",
    "\n",
    "Le terme de représentation *distribuées* est très général, mais correspond à que l'on cherche à obtenir. L'enjeu est donc de pouvoir construire, automatiquement, de telles représentations.\n",
    "\n",
    "**Idée sous-jacente**: Elle est basée sur l'hypothèse distributionelle: les informations contextuelles suffisent à obtenir une représentation viable d'objets linguistiques.\n",
    " - *“For a large class of cases [...] the  meaning  of a word is  its  use in the  language.”* Wittgenstein (Philosophical Investigations, 43 - 1953)\n",
    " - *“You shall know a word by the company it keeps”*, Firth (\"A synopsis of linguistic theory 1930-1955.\" - 1957)\n",
    "\n",
    "Ainsi, on peut caractériser un mot par les mots qui l'accompagnent, via des comptes de co-occurences. Deux mots ayant un sens similaire auront une distribution contextuelle similaire et auront donc plus de chance d'apparaître dans des contextes similaires. Cette hypothèse peut servir de justification à l'application de statistiques à la sémantique (extraction d'information, analyse sémantique). Elle permet aussi une certaine forme de généralisation: on peut supposer que les informations que l'on a à propos d'un mot se généraliseront aux mots à la distribution similaire. \n",
    "\n",
    "**Motivation**: On cherche à obtenir des représentations distribuées pour pouvoir, de manière **efficace**:\n",
    "- Directement réaliser une analyse sémantique de surface.\n",
    "- S'en servir comme source d'informations pour d'autres modèles et applications liées au language, notamment pour l'analyse de sentiments. \n",
    "\n",
    "\n",
    "**Terminologie**: Attention à ne pas confondre l'idée de représentation *distribuée* et *distributionelle*. Le second indique en général (pour les mots) que la représentation a été obtenue strictement à partir de comptes de co-occurences, alors qu'on pourra utiliser des informations supplémentaires (labels de documents, tags de partie du discours, ...) pour construire des représentations distribuées. \n",
    "Les modèles qui permettent de construire ces représentations denses, sous forme de vecteurs, sont souvent appellés *vector spaces models*. On appelle aussi régulièrement ces représentations des *word embeddings*, car les mots sont embarqués (*embedded*) dans un espace vectoriel. En Français, on rencontre souvent le terme *plongements de mots* ou *plongements lexicaux*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtenir une représentation: comptes d'occurences et de co-occurences\n",
    "\n",
    "Selon le type de corpus dont on dispose, on pourra obtenir différents types d'informations distributionelles. Si l'on a accès à une collection de documents, on pourra ainsi choisir de compter le nombre d'occurence de chaque mot dans chacun des documents, pour obtenir une matrice $mots \\times documents$: c'est sur ce principe qu'est construit **Tf-Idf**. On va maintenant s'intéresser à un cas plus général: on dispose d'une grande quantité de données sous forme de texte, et on cherche à obtenir des représentations de mots sous forme de vecteurs de taille réduite, sans avoir besoin d'un découpage en documents ou catégories. \n",
    "\n",
    "Supposons qu'on dispose d'un corpus contenant $T$ mots différents. On va construire une matrice $\\mathbf{M}$ de taille $T \\times T$ qui contiendra le nombre de co-occurences entre les mots. Il y aura différents facteurs à considérer lors de la construction de cette matrice: \n",
    "- Comment définir le 'contexte' d'un mot, qui permettra de dire que les termes qu'il contient co-occurent avec ce mot ? On pourra choisir d'utiliser différentes échelles: le document, la phrase, le groupe nominal, ou tout simplement une fenêtre de $k$ mots, selon les informations que l'on cherche à capturer.\n",
    "*Encore une fois, si par exemple notre corpus est divisé en $D$ documents, on pourra même s'intéresser aux liens distributionnels entre mots et documents: chacun de ces $D$ documents agira comme un \"contexte\", et on construit une matrice d'occurences $\\mathbf{M}$ de dimension $T \\times D$, où $\\mathbf{M}_{t,d}$ contient le nombre d'occurences du mot $t$ dans le document $d$.* \n",
    "- Comment quantifier l'importance des comptes ? Par exemple, on pourra donner un poids décroissant à une co-occurence selon la distance entre les deux mots concernés ($\\frac{1}{d+1}$ pour une séparation par $d$ mots).\n",
    "- Faut-il garder tous les mots qui apparaissent dans le corpus ? En général, non. On verra que pour les grands corpus, le nombre de mots différents $T$ est énorme. Deuxièmement, même si le nombre de mots est raisonnable, on ne possèdera que très peu d'information distributionelle sur les mots les plus rares, et la représentation obtenue sera à priori de mauvaise qualité. Il faudra se poser la question de comment filtrer ces mots, et de comment traiter les mots qu'on choisit de ne pas représenter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procédure\n",
    "\n",
    "Pour construire la matrice, on va dans un premier temps recueillir la liste des mots différents (ou le *vocabulaire* $V$) qui apparaissent dans le corpus sous forme de dictionaire {mots -> index}\n",
    "Puis, pour chaque terme $w$ du corpus,\n",
    "- On récupère l'index $i$ correspondant à l'aide de $V$\n",
    "- Pour chaque terme $w'$ du contexte de $w$, \n",
    "  + On récupère l'index $j$ correspondant à l'aide de $V$\n",
    "  + On incrémente $\\mathbf{M}_{i,j}$ par le poids correspondant, ou par $1$. \n",
    "  \n",
    "La procédure est très proche de celle qu'on a suivi au TP précédent, excepté qu'il faut maintenant compter les mots suivant leur apparition \n",
    "  \n",
    "#### Exemple\n",
    "\n",
    "On considère le corpus suivant: \n",
    "\n",
    "*I walked down down the boulevard. I walked down the avenue. I ran down the boulevard. I walk down the city. I walk down the the avenue.*\n",
    "\n",
    "On choisit de définir le contexte d'un mot comme la phrase à laquelle il appartient, et de ne pas utiliser de poids. \n",
    "On obtient la matrice suivante: \n",
    "\n",
    "|               | I | the | down | walked | boulevard | avenue | walk | ran | city |\n",
    "|---------------|---|-----|------|--------|-----------|--------|------|-----|------|\n",
    "| I             | 0 |      6 |    6 |   2 |         2 |      2 |   2 |    1 |    1 |\n",
    "| the           | 6 |      2 |    7 |   2 |         2 |      3 |   3 |    1 |    1 |\n",
    "| down          | 6 |      7 |    2 |   3 |         3 |      2 |   2 |    1 |    1 |\n",
    "| walked        | 2 |      2 |    3 |   0 |         1 |      1 |   0 |    0 |    0 |\n",
    "| boulevard     | 2 |      2 |    3 |   1 |         0 |      0 |   0 |    1 |    0 |\n",
    "| avenue        | 2 |      3 |    2 |   1 |         0 |      0 |   1 |    0 |    0 |\n",
    "| ran           | 2 |      3 |    2 |   0 |         0 |      1 |   0 |    0 |    1 |\n",
    "| walk          | 1 |      1 |    1 |   0 |         1 |      0 |   0 |    0 |    0 |\n",
    "| city          | 1 |      1 |    1 |   0 |         0 |      0 |   1 |    0 |    1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenir un Vocabulaire:\n",
    "\n",
    "Cette fois, on va implémenter séparément une fonction retournant le vocabulaire. Il faudra ici pouvoir contrôler sa taille, que ce soit en indiquant un nombre maximum de mots, ou un nombre minimum d'occurences pour qu'on prenne en compte les mots. On ajoute, à la fin, un mot \"inconnu\" qui remplacera tous les mots qui n'apparaissent pas dans notre vocabulaire 'limité'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def vocabulary(corpus, count_threshold=1, voc_threshold=0):\n",
    "    \"\"\"    \n",
    "    Function using word counts to build a vocabulary\n",
    "    Params:\n",
    "        corpus (list of list of strings): corpus of sentences\n",
    "        count_threshold (int): number of occurences necessary for a word to be included in the vocabulary\n",
    "        voc_threshold (int): maximum size of the vocabulary - 0 (default) indicates there is no max\n",
    "    Returns:\n",
    "        vocabulary (dictionary): keys: list of distinct words across the corpus\n",
    "                                 values: indexes corresponding to each word sorted by frequency \n",
    "        vocabulary_word_counts (dictionary): keys: list of distinct words across the corpus\n",
    "                                             values: word counts in the corpus\n",
    "    \"\"\"\n",
    "    \n",
    "    # On parcourt tous les textes du corpus 'corpus' et tous les mots de chaque texte 'text'..\n",
    "    # Pour chaque mot, on distingue le cas où il est déjà présent ou non dans le dictionnaire.\n",
    "    \n",
    "    word_counts = {}\n",
    "    for text in corpus:\n",
    "        # On pourrait utiliser `word_tokenize`\n",
    "        for word in clean_and_tokenize(text):\n",
    "            # 'word' est déjà présent dans 'word_counts'\n",
    "            if word in word_counts:\n",
    "                word_counts[word] += 1\n",
    "            # 'word' n'est pas encore présent dans 'word_counts'\n",
    "            else:\n",
    "                word_counts[word] = 1\n",
    "    \n",
    "    # On ne conserve que les mots dont l'occurence est supérieure à 'count_threshold'\n",
    "    filtered_word_counts = {w : word_counts[w] for w in word_counts if word_counts[w] >= count_threshold} \n",
    "   \n",
    "\n",
    "    vocabulary_word_counts = dict()\n",
    "    # Si 'voc_threshold est nul', on conserve tous les mots, il suffit de trier 'filtered_word_counts'\n",
    "    if voc_threshold == 0:\n",
    "        for k, v in sorted(filtered_word_counts.items(), key=lambda x: x[1], reverse = True):\n",
    "            vocabulary_word_counts[k] = v\n",
    "        # S'il n'existe pas, on crée 'UNK'\n",
    "        if 'UNK' not in vocabulary_word_counts:\n",
    "            vocabulary_word_counts['UNK'] = 0\n",
    "            \n",
    "    else:\n",
    "        # Sinon, on crée 'tmp', qui correspond à 'filtered_word_counts' ordonné\n",
    "        tmp = {}\n",
    "        for k, v in sorted(filtered_word_counts.items(), key=lambda x: x[1], reverse = True):\n",
    "            tmp[k] = v\n",
    "        \n",
    "        # On va incrémenter négativement 'voc_thr'\n",
    "        voc_thr = voc_threshold-1\n",
    "        # On parcourt le dictionnaire ordonné 'tmp'\n",
    "        for word in tmp:\n",
    "            # Si on est encore dans la limite du vocabulaire\n",
    "            if voc_thr > 0:\n",
    "                voc_thr -= 1\n",
    "                # On ajoute le mot 'word' comme clef et son occurence comme valeur au dictionnaire\n",
    "                vocabulary_word_counts[word] = tmp[word]\n",
    "            # Sinon ce sera le mot inconnu 'UNK'\n",
    "            else:\n",
    "                # S'il existe déjà on augmente l'occurence des mots inconnus\n",
    "                if 'UNK' in vocabulary_word_counts:\n",
    "                    vocabulary_word_counts['UNK'] += tmp[word]\n",
    "                # Sinon on crée l'entrée 'UNK'\n",
    "                else:\n",
    "                    vocabulary_word_counts['UNK'] = tmp[word]\n",
    "        # S'il n'existe pas, on crée 'UNK'\n",
    "        if 'UNK' not in vocabulary_word_counts:\n",
    "            vocabulary_word_counts['UNK'] = 0\n",
    "    \n",
    "    # On parcourt le dictionnaire 'vocabulary_word_counts' qui est trié par construction afin de construire le\n",
    "    # dictionnaire 'vocabulary' en incrémentant une variable 'index' qui correspond à un rang selon la fréquence\n",
    "    \n",
    "    vocabulary = {}\n",
    "    index = 0\n",
    "    for elt in vocabulary_word_counts:\n",
    "        vocabulary[elt] = index\n",
    "        index += 1\n",
    "        \n",
    "    return vocabulary, vocabulary_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'down': 0, 'the': 1, 'UNK': 2}\n",
      "{'down': 6, 'the': 6, 'UNK': 0}\n",
      "{'down': 0, 'the': 1, 'walked': 2, 'boulevard': 3, 'avenue': 4, 'walk': 5, 'ran': 6, 'city': 7, 'UNK': 8}\n",
      "{'down': 6, 'the': 6, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1, 'UNK': 0}\n"
     ]
    }
   ],
   "source": [
    "# Example for testing:\n",
    "\n",
    "corpus = ['I walked down down the boulevard',\n",
    "          'I walked down the avenue',\n",
    "          'I ran down the boulevard',\n",
    "          'I walk down the city',\n",
    "          'I walk down the the avenue']\n",
    "\n",
    "voc, counts = vocabulary(corpus, count_threshold = 3)\n",
    "print(voc)\n",
    "print(counts)\n",
    "\n",
    "# We expect something like this:\n",
    "#  {'down': 0, 'the': 1, 'i': 2, 'UNK': 3}\n",
    "#  {'down': 6, 'the': 6, 'i': 5, 'UNK': 0}\n",
    "\n",
    "voc, counts = vocabulary(corpus)\n",
    "print(voc)\n",
    "print(counts)\n",
    "\n",
    "# We expect something like this:\n",
    "#  {'down': 0, 'the': 1, 'i': 2, 'walked': 3, 'boulevard': 4, 'avenue': 5, 'walk': 6, 'ran': 7, 'city': 8, 'UNK': 9}\n",
    "#  {'down': 6, 'the': 6, 'i': 5, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1, 'UNK': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ici, on ne retrouve pas exactement ce qui est suggéré : cependant, on peut vérifier qu'en remplaçant `clean_and_tokenize` par `nltk.tokenize.word_tokenize` dans la fonction `vocabulary`, on obtient exactement le résultat suivant :\n",
    "\n",
    "```css\n",
    "{'down': 0, 'the': 1, 'i': 2, 'UNK': 3}\n",
    "{'down': 6, 'the': 6, 'i': 5, 'UNK': 0}\n",
    "{'down': 0, 'the': 1, 'i': 2, 'walked': 3, 'boulevard': 4, 'avenue': 5, 'walk': 6, 'ran': 7, 'city': 8, 'UNK': 9}\n",
    "{'down': 6, 'the': 6, 'i': 5, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1, 'UNK': 0}\n",
    "```\n",
    "\n",
    "- On préférera cependant l'implémentation de  `vocabulary` avec `clean_and_tokenize` pour la suite du TP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenir les co-occurences:\n",
    "\n",
    "La fonction prend en entrée le corpus (une liste de strings, correspondant aux documents, ou phrases) et un vocabulaire, ainsi que la taille de la fenêtre de contexte. On pourra aussi implémenter la solution la plus simple - que le contexte d'un mot soit le reste du document duquel il provient. \n",
    "Enfin, on pourra implémenter la possibilité de faire décroitre linéairement l'importance d'un mot du contexte à mesure qu'on s'éloigne du mot d'origine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurence_matrix(corpus, vocabulary, window=0, distance_weighting=False):\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        corpus (list of list of strings): corpus of sentences\n",
    "        vocabulary (dictionary): words to use in the matrix\n",
    "        window (int): size of the context window; when 0, the context is the whole sentence\n",
    "        distance_weighting (bool): indicates if we use a weight depending on the distance between words for co-oc counts\n",
    "    Returns:\n",
    "        matrix (array of size (len(vocabulary), len(vocabulary))): the co-oc matrix, using the same ordering as the vocabulary given in input    \n",
    "    \"\"\" \n",
    "    l = len(vocabulary)\n",
    "    M = np.zeros((l,l))\n",
    "    for sent in corpus:\n",
    "        # Obtenir la phrase:\n",
    "        sent = word_tokenize(sent)\n",
    "        # Obtenir les indexs de la phrase grace au vocabulaire: \n",
    "        sent_idx = []\n",
    "        ind_unk = len(vocabulary)-1\n",
    "        for w in sent:\n",
    "            if w in vocabulary:\n",
    "                sent_idx.append(vocabulary[w])\n",
    "            else:\n",
    "                sent_idx.append(ind_unk)\n",
    "        # Parcourir les indexs de la phrase et ajouter 1 / dist(i,j) à M[i,j] si les mots d'index i et j apparaissent dans la même fenêtre. \n",
    "        for i, idx_i in enumerate(sent_idx):\n",
    "            # On vérifie que le mot est reconnu par le vocabulaire:\n",
    "            if idx_i > -1:\n",
    "                # Si on considère un contexte limité:\n",
    "                if window > 0:\n",
    "                    # On crée une liste qui contient les indexs de la fenêtre à gauche de l'index courant 'idx_i'\n",
    "                    l_ctx_idx = [sent_idx[jj] for jj in range(max(i-window, 0),i)]\n",
    "                # Si on considère que le contexte est la phrase entière:\n",
    "                else:\n",
    "                    # La liste qui contient le contexte à gauche du mot est plus facile à créer:\n",
    "                    l_ctx_idx = [sent_idx[jj] for jj in range(len(sent_idx)) if jj != i]\n",
    "                # On parcourt cette liste et on update M[i,j]:    \n",
    "                for j, idx_j in enumerate(l_ctx_idx):\n",
    "                    # ... en s'assurant que le mot correspondant à 'idx_j' est reconnu par le vocabulaire\n",
    "                    if idx_j > -1:\n",
    "                        # Calcul du poids:\n",
    "                        if distance_weighting:\n",
    "                            # Le poids est l'inverse de la distance entre i et j, le facteur 1/2 est chosi car\n",
    "                            # les poids sont affectés deux fois par construction\n",
    "                            # Le poids est de l'ordre de 1 / (d+1) où d est la distance entre 'i' et 'j'\n",
    "                            weight = .5 / (np.abs(i - j) + 1)\n",
    "                        else:\n",
    "                            weight = 0.5\n",
    "                        M[idx_i, idx_j] += weight * 1.0\n",
    "                        M[idx_j, idx_i] += weight * 1.0\n",
    "    return M  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 7. 3. 3. 2. 2. 1. 1. 6.]\n",
      " [7. 2. 2. 2. 3. 3. 1. 1. 6.]\n",
      " [3. 2. 0. 1. 1. 0. 0. 0. 2.]\n",
      " [3. 2. 1. 0. 0. 0. 1. 0. 2.]\n",
      " [2. 3. 1. 0. 0. 1. 0. 0. 2.]\n",
      " [2. 3. 0. 0. 1. 0. 0. 1. 2.]\n",
      " [1. 1. 0. 1. 0. 0. 0. 0. 1.]\n",
      " [1. 1. 0. 0. 0. 1. 0. 0. 1.]\n",
      " [6. 6. 2. 2. 2. 2. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(co_occurence_matrix(corpus, voc, 0, False))\n",
    "# Attention: les résultats sont différents de l'exemple plus haut car le vocabulaire n'est pas exactement le même: vérifiez par vous même."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application à un vrai jeu de données\n",
    "\n",
    "On va chercher à obtenir ces comptes pour les données **imdb**.\n",
    "\n",
    "#### Etude rapide des données\n",
    "\n",
    "On voudrait ici, avant de procéder, avoir une idée de ce que contiennent ces critiques de films. On va donc obtenir le vocabulaire (entier) et représenter les fréquences des mots, dans l'ordre (attention, il faudra utiliser une échelle logarithmique): on devrait retrouver la loi de Zipf. Cela nous permettra d'avoir une idée de la taille du vocabulaire qu'on pourra choisir : il s'agit de réaliser un compromis entre les ressources nécessaires (taille des objets en mémoire) et quantité d'informations qu'on peut en tirer (les mots rares peuvent apporter beaucoup d'informations, mais il est difficile d'en apprendre de bonnes représentations,car ils sont rares !)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = texts\n",
    "\n",
    "vocab, word_counts = vocabulary(corpus)\n",
    "rank_counts = {w:[vocab[w], word_counts[w]] for w in vocab}\n",
    "rank_counts_array = np.array(list(rank_counts.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.title('Word counts versus rank')\n",
    "# plt.scatter(rank_counts_array[:,0], rank_counts_array[:,1], marker=\"+\")\n",
    "plt.plot(rank_counts_array[:,0], rank_counts_array[:,1], linewidth=2, color = 'blue')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Word Rank')\n",
    "plt.ylabel('Word counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On retrouve bien la représentation de la loi de Zipf : la fréquence d'un mot est approximativement inversement proportionnelle à son rang. La représentation à angles rectangulaires pour les mots de rangs élevés est liée à l'échelle logarihmique qui impose cette représentation peu esthétique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 105978\n",
      "Part of the corpus by taking the \"x\" most frequent words:\n",
      "5000 : 0.87\n",
      "15000 : 0.95\n",
      "25000 : 0.97\n",
      "35000 : 0.98\n",
      "45000 : 0.98\n",
      "55000 : 0.99\n",
      "65000 : 0.99\n",
      "75000 : 0.99\n",
      "85000 : 1.00\n",
      "95000 : 1.00\n",
      "105000 : 1.00\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size: %i' % len(vocab))\n",
    "print('Part of the corpus by taking the \"x\" most frequent words:')\n",
    "for i in range(5000, len(vocab), 10000):\n",
    "    print('%i : %.2f' % (i, np.sum(rank_counts_array[:i, 1]) / np.sum(rank_counts_array[:,1]) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultat de l'analyse: on peut se contenter d'un vocabulaire de 10000, voire 5000 mots - c'est important, car cela va déterminer la taille des objets que l'on va manipuler. On va maintenant recréer la matrice de co-occurence avec différents paramètres. Cela peut-être long: si cela pose problème, travaillez avec un vocabulaire plus réduit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 5000)\n",
      "(5000, 5000)\n"
     ]
    }
   ],
   "source": [
    "vocab_5k, word_counts_5k = vocabulary(corpus, 0, 5000)\n",
    "M5dist = co_occurence_matrix(corpus, vocab_5k, window=5, distance_weighting=True)\n",
    "M20 = co_occurence_matrix(corpus, vocab_5k, window=20, distance_weighting=False)\n",
    "print(M5dist.shape)\n",
    "print(M20.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411\n",
      "[ 9.60200929  2.65424656  1.91615234 ...  0.          0.\n",
      " 79.77124659]\n",
      "[ 1035.5   521.5   251.5 ...     0.      0.  11185.5]\n"
     ]
    }
   ],
   "source": [
    "print(vocab_5k['cinema'])\n",
    "print(M5dist[411])\n",
    "print(M20[411])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison de vecteurs\n",
    "\n",
    "On peut se servir de ces vecteurs de très grande taille pour une analyse sémantique très basique: par exemple, en cherchant les plus proches voisins d'un mot. Cependant, il faudra faire attention aux distances qu'on utilise, liées à certaines métriques (Euclidiennes, Cosine) ou éventuellement d'autres liées à l'appartenance aux ensembles (Matching, Jaccard). La normalisation des vecteurs peut aussi jouer un rôle. Dans tous les cas, il faut bien faire attention à ne pas sur-interprêter ce type de résultats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec un contexte large, sans prendre en compte la distance entre les mots:\n",
      "Plus proches voisins de good selon la distance 'euclidean': \n",
      "[['some', 'more', 'very', 'would', 'what', 'there', 'which', 'can', 'out']]\n",
      "Plus proches voisins de good selon la distance 'cosine': \n",
      "[['great', 'decent', 'really', 'just', 'still', 'very', 'not', 'nice', 'lot']]\n",
      "\n",
      "Avec un contexte plus petit, et en réduisant l'impact des paires de mots selon leur distance:\n",
      "Plus proches voisins de good selon la distance 'euclidean': \n",
      "[['very', 'really', 'out', 'just', 'had', 'who', 'first', 'story', 'what']]\n",
      "Plus proches voisins de good selon la distance 'cosine': \n",
      "[['great', 'bad', 'funny', 'interesting', 'very', 'decent', 'well', 'the', 'little']]\n"
     ]
    }
   ],
   "source": [
    "def euclidean(u, v):\n",
    "    return np.linalg.norm(u-v)\n",
    "\n",
    "def length_norm(u):\n",
    "    return u / np.sqrt(u.dot(u))\n",
    "\n",
    "def cosine(u, v):\n",
    "    return 1.0 - length_norm(u).dot(length_norm(v))\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def print_neighbors(distance, voc, co_oc, mot, k=10):  \n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    inv_voc = {id: w for w, id in voc.items()}\n",
    "    neigh = NearestNeighbors(k, algorithm='brute', metric=distance)\n",
    "    neigh.fit(co_oc) \n",
    "    dist, ind = neigh.kneighbors([co_oc[voc[mot]]])\n",
    "    print(\"Plus proches voisins de %s selon la distance '%s': \" % (mot, distance.__name__))\n",
    "    print([[inv_voc[i] for i in s[1:]] for s in ind])\n",
    "    \n",
    "print(\"Avec un contexte large, sans prendre en compte la distance entre les mots:\")    \n",
    "print_neighbors(euclidean, vocab_5k, M20, 'good')\n",
    "print_neighbors(cosine, vocab_5k, M20, 'good')\n",
    "print(\"\")\n",
    "print(\"Avec un contexte plus petit, et en réduisant l'impact des paires de mots selon leur distance:\")    \n",
    "print_neighbors(euclidean, vocab_5k, M5dist, 'good')\n",
    "print_neighbors(cosine, vocab_5k, M5dist, 'good') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalisation**: Très simple; il s'agit d'annuler l'influence de la magnitude des comptes sur la représentation.\n",
    "\n",
    "$$\\mathbf{m_{normalized}} = \\left[ \n",
    "   \\frac{m_{1}}{\\sum_{i=1}^{n}m_{i}}, \n",
    "   \\frac{m_{2}}{\\sum_{i=1}^{n}m_{i}}, \n",
    "   \\ldots\n",
    "   \\frac{m_{n}}{\\sum_{i=1}^{n}m_{i}}, \n",
    "\\right]$$\n",
    " \n",
    "**Pointwise Mutual Information**: Il s'agit d'évaluer à quel point la co-occurence des deux termes est *inattendue*. En effet, cette mesure correspond au ratio de la probabilité jointe des deux mots et du produit de leur probabilités individuelles:\n",
    "$$\n",
    "\\text{PMI}(x,y) = \\log \\left( \\frac{P(x,y)}{P(x)P(y)} \\right)\n",
    "$$\n",
    "La probabilité jointe des deux mots correspond au nombre de fois ou on les observe ensemble, divisé par le nombre total de co-occurences du corpus: \n",
    "$$ P(\\mathbf{M},w_{1},w_{2}) = \\frac{M_{w_{1},w_{2}}}{\\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j}} $$\n",
    "La probabilité individuelle d'un mot correspond simplement à sa fréquence, que l'on peut calculer en comptant toutes les co-occurences ou ce mot apparaît:\n",
    "$$ P(\\mathbf{M},w) = \\frac{\\sum_{j=1}^{m} M_{w,j}}{\\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j}} $$\n",
    "Ainsi,\n",
    "$$ \n",
    "\\text{PMI}(\\mathbf{M},w_{1},w_{2}) = \\log  \\frac{M_{w_{1},w_{2}} \\times \\left( \\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j} \\right)}{\\left( \\sum_{j=1}^{n} M_{w_{1},j} \\right) \\times \\left( \\sum_{i=1}^{n}M_{i,w_{2}} \\right)} \n",
    "$$\n",
    "On calcule ainsi le décalage entre l'observation que l'on a fait dans notre corpus et la fréquence d'apparition de ces termes si on les considère indépendant - c'est à dire qu'on suppose que leur co-occurence est une coïncidence.\n",
    "\n",
    "Le principal problème avec cette mesure est qu'elle n'est pas adaptée au cas où l'on observe aucune co-occurence. Puisque la PMI est censée renvoyer une quantité positive si l'on observe plus de co-occurences que prévu, et négative si l'on en observe moins, on ne peut pas choisir de remplacer $\\log(0)$ par $0$. Une solution couramment utilisée est d'utiliser la **Positive PMI**, qui fixe toutes les valeurs négatives à $0$.\n",
    " \n",
    " $$\\text{PPMI}(\\mathbf{M},w_{1},w_{2}) = \n",
    " \\begin{cases}\n",
    " \\text{PMI}(\\mathbf{M},w_{1},w_{2}) & \\textrm{if } \\text{PMI}(\\mathbf{M},w_{1},w_{2}) > 0 \\\\\n",
    " 0 & \\textrm{otherwise}\n",
    " \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(co_oc, positive=True):\n",
    "    sum_vec = co_oc.sum(axis=0)\n",
    "    sum_tot = sum_vec.sum()\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        pmi = np.log((co_oc * sum_tot) / (np.outer(sum_vec, sum_vec)))                   \n",
    "    pmi[np.isinf(pmi)] = 0.0  # log(0) = 0\n",
    "    pmi[np.isnan(pmi)] = 0.0\n",
    "    if positive:\n",
    "        pmi[pmi < 0] = 0.0\n",
    "    return pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec la PPMI5:\n",
      "Plus proches voisins de good selon la distance 'euclidean': \n",
      "[['UNK', 'rochester', 'curtis', 'arnold', 'better<', 'dorothy', 'movies<', 'stanwyck', 'lloyd']]\n",
      "Plus proches voisins de good selon la distance 'cosine': \n",
      "[['decent', 'great', 'and', 'interesting', 'some', 'but', 'pretty', 'bad', 'are']]\n",
      "Avec la PPMI20:\n",
      "Plus proches voisins de good selon la distance 'euclidean': \n",
      "[['UNK', 'wanna', 'rachel', 'hitchcock', 'hoffman', 'michelle', 'elizabeth', '>theres', 'again<']]\n",
      "Plus proches voisins de good selon la distance 'cosine': \n",
      "[['some', 'well', 'but', 'was', 'this', 'great', 'acting', 'and', 'bad']]\n"
     ]
    }
   ],
   "source": [
    "PPMI5 = pmi(M5dist)\n",
    "PPMI20 = pmi(M20)\n",
    "\n",
    "print(\"Avec la PPMI5:\")    \n",
    "print_neighbors(euclidean, vocab_5k, PPMI5, 'good')\n",
    "print_neighbors(cosine, vocab_5k, PPMI5, 'good')\n",
    "print(\"Avec la PPMI20:\")   \n",
    "print_neighbors(euclidean, vocab_5k, PPMI20, 'good')\n",
    "print_neighbors(cosine, vocab_5k, PPMI20, 'good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**: Comme on l'a déjà vu, il s'agit du produit de la fréquence du terme (TF) et de sa fréquence inverse dans les documents (IDF). \n",
    "Cette méthode est habituellement utilisée pour extraire l'importance d'un terme $i$ dans un document $j$ relativement au reste du corpus, à partir d'une matrice $termes \\times documents$. Ainsi, pour une matrice $\\mathbf{X}$ de $n$ termes et $d$ documents: \n",
    "\n",
    " $$\\text{TF}(X, i, j) = \\frac{X_{i,j}}{\\sum_{i=1}^{t} X_{i,j}} $$\n",
    " \n",
    " $$\\text{IDF}(X, i) = \\log\\left(\\frac{d}{|\\{j : X_{i,j} > 0\\}|}\\right)$$\n",
    " \n",
    " $$\\text{TF-IDF}(X, i, j) = \\text{TF}(X, i, j) \\cdot \\text{IDF}(X, i)$$\n",
    "\n",
    "\n",
    "On peut l'adapter à notre cas en considérant que le contexte du deuxième mot est le document. Cependant, TF-IDF est généralement plus adaptée aux matrices peu denses, puisque cette mesure pénalisera les termes qui apparaissent dans une grande partie des documents. Ainsi, l'appliquer aux co-occurences des mots les plus fréquents n'est à priori pas optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(co_oc):\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    \"\"\"\n",
    "    Inverse document frequencies applied to our co_oc matrices\n",
    "    \"\"\"\n",
    "    # IDF\n",
    "    d = float(co_oc.shape[1])\n",
    "    in_doc = co_oc.astype(bool).sum(axis=1)\n",
    "    #in_doc[in_doc==0] = 0.01\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        idfs = np.log(d / in_doc)\n",
    "    idfs[np.isinf(idfs)] = 0.0  # log(0) = 0\n",
    "    # TF\n",
    "    sum_vec = co_oc.sum(axis=0)\n",
    "    tfs = co_oc / sum_vec\n",
    "    res = (tfs.T * idfs).T\n",
    "    res[np.isnan(res)] = 0.0\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec TF-IDF:\n",
      "Plus proches voisins de good selon la distance 'euclidean': \n",
      "[['robin', 'sudden', 'necessary', 'worried', 'conclusion', 'questionable', 'credible', 'dub', 'overlook']]\n",
      "Plus proches voisins de good selon la distance 'cosine': \n",
      "[['UNK', 'pretty', 'and', 'the', 'but', 'dalton', 'that', 'was', 'very']]\n"
     ]
    }
   ],
   "source": [
    "TFIDF5 = tfidf(M5dist)\n",
    "\n",
    "print(\"Avec TF-IDF:\")    \n",
    "print_neighbors(euclidean, vocab_5k, TFIDF5, 'good')\n",
    "print_neighbors(cosine, vocab_5k, TFIDF5, 'good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests pour différents mots : _good_ -  _disgusting_ - _beautiful_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec la PPMI5:\n",
      "Plus proches voisins de good selon la distance 'cosine': \n",
      "[['decent', 'great', 'and', 'interesting', 'some', 'but', 'pretty', 'bad', 'are']]\n",
      "Avec la PPMI20:\n",
      "Plus proches voisins de good selon la distance 'cosine': \n",
      "[['some', 'well', 'but', 'was', 'this', 'great', 'acting', 'and', 'bad']]\n",
      "Avec TF-IDF:\n",
      "Plus proches voisins de good selon la distance 'cosine': \n",
      "[['UNK', 'pretty', 'and', 'the', 'but', 'dalton', 'that', 'was', 'very']]\n"
     ]
    }
   ],
   "source": [
    "w = 'good'\n",
    "\n",
    "print(\"Avec la PPMI5:\")    \n",
    "print_neighbors(cosine, vocab_5k, PPMI5, w)\n",
    "\n",
    "print(\"Avec la PPMI20:\")   \n",
    "print_neighbors(cosine, vocab_5k, PPMI20, w)\n",
    "\n",
    "print(\"Avec TF-IDF:\")\n",
    "print_neighbors(cosine, vocab_5k, TFIDF5, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec la PPMI5:\n",
      "Plus proches voisins de disgusting selon la distance 'cosine': \n",
      "[['offensive', 'crude', 'hilarious', 'downright', 'and', 'nudity', 'jokes', 'sex', 'sometimes']]\n",
      "Avec la PPMI20:\n",
      "Plus proches voisins de disgusting selon la distance 'cosine': \n",
      "[['nudity', 'violence', 'sex', 'disturbing', 'sick', 'stupid', 'gore', 'images', 'blood']]\n",
      "Avec TF-IDF:\n",
      "Plus proches voisins de disgusting selon la distance 'cosine': \n",
      "[['billy', 'jokes', 'inane', 'cuts', 'eating', 'sees', 'wind', 'countries', 'american']]\n"
     ]
    }
   ],
   "source": [
    "w = 'disgusting'\n",
    "\n",
    "print(\"Avec la PPMI5:\")\n",
    "print_neighbors(cosine, vocab_5k, PPMI5, w)\n",
    "\n",
    "print(\"Avec la PPMI20:\")\n",
    "print_neighbors(cosine, vocab_5k, PPMI20, w)\n",
    "\n",
    "print(\"Avec TF-IDF:\")\n",
    "print_neighbors(cosine, vocab_5k, TFIDF5, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec la PPMI5:\n",
      "Plus proches voisins de beautiful selon la distance 'cosine': \n",
      "[['gorgeous', 'and', 'breathtaking', 'scenery', 'cinematography', 'stunning', 'shots', 'with', 'beauty']]\n",
      "Avec la PPMI20:\n",
      "Plus proches voisins de beautiful selon la distance 'cosine': \n",
      "[['cinematography', 'beauty', 'and', 'gorgeous', 'scenery', 'stunning', 'wonderful', 'score', 'atmosphere']]\n",
      "Avec TF-IDF:\n",
      "Plus proches voisins de beautiful selon la distance 'cosine': \n",
      "[['surrounded', 'host', 'dozen', 'interested', 'breathtaking', 'and', 'UNK', 'reflect', 'the']]\n"
     ]
    }
   ],
   "source": [
    "w = 'beautiful'\n",
    "\n",
    "print(\"Avec la PPMI5:\")\n",
    "print_neighbors(cosine, vocab_5k, PPMI5, w)\n",
    "\n",
    "print(\"Avec la PPMI20:\")\n",
    "print_neighbors(cosine, vocab_5k, PPMI20, w)\n",
    "\n",
    "print(\"Avec TF-IDF:\")\n",
    "print_neighbors(cosine, vocab_5k, TFIDF5, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On remarque que pour les trois méthodes, les résultats obtenus avec la métrique euclidienne sont aberrants. On ne s'intéresse donc désormais qu'aux résultats obtenus avec la métrique cosinus.\n",
    "\n",
    "- On constate que les résultats obtenus avec la PPMI sont effectivement bien plus pertinents que ceux obtenus avec TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de co-occurences : Réduction de dimension\n",
    "\n",
    "#### Motivation\n",
    "\n",
    "Il s'agit non seulement de réduire la taille de données (ainsi, on traitera des vecteurs de dimension réduite, plutôt que de travailler avec des vecteurs de la taille du vocabulaire) mais aussi de mettre en évidence des relations de plus haut niveau entre les mots: en réduisant leurs représentations aux dimensions qui *les plus importantes* des données, on se retrouve à *généraliser* certaines propriétés entre les mots.\n",
    "\n",
    "#### Réduction de dimension via SVD \n",
    "\n",
    "Une matrice est une transformation linéaire: y appliquer une SVD, c'est décomposer notre transformation linéaire en un produit de transformations linéaires de différents types. Il va s'agir d'effectuer un changement de base, et de replacer nos données dans un espace ou chacune des coordonnées sont inchangées par la transformation effectuée. Ainsi, on décompose la matrice $\\mathbf{M}$ en trois matrices:\n",
    "\n",
    "$$ \\mathbf{M} = \\mathbf{U} \\mathbf{\\lambda} \\mathbf{V}^{\\text{T}} $$\n",
    "\n",
    "Les matrices $\\mathbf{U}$, $\\mathbf{\\lambda}$, et $\\mathbf{V}$ ont les propriétés suivantes:\n",
    "- $\\mathbf{U}$ et $\\mathbf{V}$ sont des matrices orthogonales ($\\mathbf{U}^{\\text{T}} = \\mathbf{U}^{-1}$ et $\\mathbf{V}^{\\text{T}} = \\mathbf{V}^{-1}$). Elles contiennent les vecteurs propres à gauche et à droite de $\\mathbf{M}$.\n",
    "- $\\mathbf{\\lambda}$ est une matrice diagonale: attention, elle n'est pas forcément carrée. Les coefficients de la diagonale sont les valeurs propres de $\\mathbf{M}$.\n",
    "\n",
    "Ainsi, les dmensions *les plus importantes* correspondent aux plus grandes valeurs propres. Réduire nos données à une dimension $k$ correspond à ne garder que les vecteurs correspondant aux $k$ premières valeurs propres - et cela revient à prendre les $k$ premiers vecteurs de la matrice $U$. \n",
    "On utilise ici ```TruncatedSVD``` du package ```scikit-learn```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 300)\n",
      "Plus proches voisins de good selon la distance 'euclidean': \n",
      "[['very', 'really', 'out', 'just', 'had', 'who', 'first', 'story', 'what']]\n",
      "Plus proches voisins de good selon la distance 'cosine': \n",
      "[['great', 'bad', 'funny', 'interesting', 'decent', 'very', 'well', 'similar', 'obviously']]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=300)\n",
    "SVDEmbeddings = svd.fit_transform(M5dist)\n",
    "print(SVDEmbeddings.shape)\n",
    "SVDEmbeddings[vocab_5k['UNK']]\n",
    "\n",
    "print_neighbors(euclidean, vocab_5k, SVDEmbeddings, 'good')\n",
    "print_neighbors(cosine, vocab_5k, SVDEmbeddings, 'good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Lorsque l'on applique cette méthode à la matrice des comptes $\\mathbf{M}$ de dimension $T \\times D$, où $\\mathbf{M}_{t,d}$ contient le nombre d'occurences du mot $t$ dans le document $d$, on obtient la méthode appellée **Latent Semantic Analysis**, pour la détection de composantes latentes (sémantiques) permettant de regrouper les documents.  \n",
    "\n",
    "#### Visualisation en deux dimensions\n",
    "\n",
    "On va maintenant utiliser **l'analyse en composantes principales** (PCA) pour visualiser nos données en 2 dimensions.  Cela revient à appliquer la SVD à la matrice de covariance des données, pour que les directions principales soient indépendantes les unes des autres et maximisent la variance des données.\n",
    "On utilise la classe ```PCA``` du package ```scikit-learn```: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nous définissions ici une fonction auxiliaire qui sera utile pour la suite. Si un mot 'word' n'est pas dans\n",
    "## le vocabulaire 'vocabulary', la fonction 'unk_word' change échange ce mot par 'UNK' (type string)\n",
    "\n",
    "def unk_word(word, vocabulary):\n",
    "    if word not in vocabulary:\n",
    "        word = 'UNK'\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "Emb = pca.fit_transform(M5dist)\n",
    "\n",
    "words = ['bad', 'good', 'best', 'worst', 'poor', 'great',\n",
    "         'dialog', 'role', 'actor', 'camera', 'scene',\n",
    "         'film', 'movie', 'oscar', 'award']\n",
    "\n",
    "words = [unk_word(word, vocab_5k) for word in words]\n",
    "\n",
    "ind_words = [vocab_5k[w] for w in words]\n",
    "x_words = [Emb[ind,0] for ind in ind_words]\n",
    "y_words = [Emb[ind,1] for ind in ind_words]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_words, y_words)\n",
    "\n",
    "for i, w in enumerate(words):\n",
    "    ax.annotate(w, (x_words[i], y_words[i]), (x_words[i] + 0.001, y_words[i] + 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Norm5 = M5dist / np.linalg.norm(M5dist, ord=2, axis=1, keepdims=True)\n",
    "Norm5[np.isnan(Norm5)] = 0\n",
    "\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "Emb = pca.fit_transform(Norm5)\n",
    "\n",
    "words = ['bad', 'good', 'best', 'worst', 'poor', 'great',\n",
    "         'dialog', 'role', 'actor', 'camera', 'scene',\n",
    "         'film', 'movie', 'oscar', 'award']\n",
    "\n",
    "words = [unk_word(word, vocab_5k) for word in words]\n",
    "\n",
    "ind_words = [vocab_5k[w] for w in words]\n",
    "x_words = [Emb[ind,0] for ind in ind_words]\n",
    "y_words = [Emb[ind,1] for ind in ind_words]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_words, y_words)\n",
    "\n",
    "for i, w in enumerate(words):\n",
    "    ax.annotate(w, (x_words[i], y_words[i]), (x_words[i] + 0.001, y_words[i] + 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtenir une représentation: algorithmes couramment utilisés\n",
    "\n",
    "L'idée, ici, est de définir un ensemble de représentations ${w_{i}}_{i=1}^{V}$, de dimension prédéfinie $d$ (ici, on travaillera avec $d = 300$), pour tous les mots $i$ du vocabulaire $V$ - puis **d'entraîner** ces représentations pour qu'elles correspondent à ce que l'on souhaite. \n",
    "\n",
    "### Glove\n",
    "\n",
    "L'objectif défini par Glove ([Pennington et al. (2014)](http://www.aclweb.org/anthology/D/D14/D14-1162.pdf)) est d'apprendre des vecteurs $w_{i}$ et $w_{k}$ de façon à ce que leur produit scalaire correspondent au logarithme de leur **Pointwise Mutual Information**: \n",
    "\n",
    "\n",
    "$$ w_{i}^\\top w_{k} = (PMI(w_{i}, w_{k}))$$\n",
    "\n",
    "\n",
    "Dans l'article, l'obtention de cet objectif est minutieusement justifié par un raisonnement sur les opérations que l'on veut effectuer avec ces vecteurs et les propriétés qu'ils devraient avoir - notamment, une symétrie entre les lignes et les colonnes (voir l'article pour plus de détails).  \n",
    "L'objectif final obtenu est le suivant, où $M$ est la matrice de co-occurences:\n",
    "\n",
    "\n",
    "$$\\sum_{i, j=1}^{|V|} f\\left(M_{ij}\\right)\n",
    "  \\left(w_i^\\top w_j + b_i + b_j - \\log M_{ij}\\right)^2$$\n",
    "  \n",
    " \n",
    "Ici, $f$ est une fonction de *mise à l'échelle* qui permet de diminuer l'importance des comptes de co-occurences les plus fréquents: \n",
    "\n",
    "\n",
    "$$f(x) \n",
    "\\begin{cases}\n",
    "(x/x_{\\max})^{\\alpha} & \\textrm{if } x < x_{\\max} \\\\\n",
    "1 & \\textrm{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "En général, on choisit $\\alpha=0.75$ et $x_{\\max} = 100$, même si ces paramètres peuvent nécessiter un changement selon les données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code suivant utilise l'API de gensim pour récupérer des représentations pré-entrainées (Il est normal que le chargement soit long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "loaded_glove_model = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut extraire la matrice des embeddings ainsi, et vérifier sa taille:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 300)\n"
     ]
    }
   ],
   "source": [
    "loaded_glove_embeddings = loaded_glove_model.vectors\n",
    "print(loaded_glove_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit donc qu'il y a $400.000$ mots représentés, et que les embeddings sont de dimension $300$. On définit une fonction qui nous renvoie, à partir du modèle chargé, le vocabulaire et la matrice des embeddings suivant les structures que l'on a utilisé auparavant. On ajoute, ici encore, un mot inconnu ```'UNK'``` au cas où se trouve dans nos données des mots qui ne font pas parti des $400.000$ mots représentés ici. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_voc_and_embeddings(glove_model):\n",
    "    voc = {word : index for word, index in enumerate(glove_model.index2word)}\n",
    "    voc['UNK'] = len(voc)\n",
    "    embeddings = glove_model.vectors\n",
    "    return voc, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_glove_voc, loaded_glove_embeddings = get_glove_voc_and_embeddings(loaded_glove_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de comparer 'à jeu égal' les représentations chargées ici et celles que l'on a produite, il faudrait utiliser le même vocabulaire. Dans ce but, je réutilise le code qui suit pour créer un vocabulaire de $5000$ mots à partir des données exactement comme hier, et j'ajoute à la fin une fonction qui renvoie la matrices des représentations chargées avec Glove pour ces $5000$ mots seulement, dans le bon ordre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_adapted_embeddings(glove_model, input_voc):\n",
    "    keys = {i: glove_model.vocab.get(w, None) for w, i in input_voc.items()}\n",
    "    index_dict = {i: key.index for i, key in keys.items() if key is not None}\n",
    "    embeddings = np.zeros((len(input_voc),glove_model.vectors.shape[1]))\n",
    "    for i, ind in index_dict.items():\n",
    "        embeddings[i] = glove_model.vectors[ind]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "GloveEmbeddings = get_glove_adapted_embeddings(loaded_glove_model, vocab_5k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction prend donc en entrée le modèle chargé à l'aide de l'API Gensim, ainsi qu'un vocabulaire que nous avons créé nous même, et renvoie la matrice d'embeddings tiré du modèle chargé, pour les mots notre vocabulaire et dans le bon ordre.\n",
    "Remarque: les mots inconnus sont représentés par le vecteur nul:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(GloveEmbeddings.shape)\n",
    "GloveEmbeddings[vocab_5k['UNK']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plus proches voisins de good selon la distance 'euclidean': \n",
      "[['better', 'well', 'always', 'really', 'sure', 'way', 'but', 'excellent', 'certainly']]\n",
      "Plus proches voisins de good selon la distance 'cosine': \n",
      "[['better', 'really', 'always', 'you', 'well', 'excellent', 'very', 'things', 'think']]\n"
     ]
    }
   ],
   "source": [
    "print_neighbors(euclidean, vocab_5k, GloveEmbeddings, 'good')\n",
    "print_neighbors(cosine, vocab_5k, GloveEmbeddings, 'good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application à l'analyse de sentiments\n",
    "\n",
    "On va maintenant utiliser ces représentations pour l'analyse de sentiments. \n",
    "Le modèle de base, comme hier, sera construit en deux étapes:\n",
    "- Une fonction permettant d'obtenir des représentations vectorielles des critiques, à partir des textes, du vocabulaire, et des représentations vectorielles des mots. Une telle fonction (à compléter ci-dessous) va associer à chaque mot d'une critique son embeddings, et créer la représentation pour l'ensemble de la phrase en sommant ces embeddings.\n",
    "- Un classifieur qui prendra ces représentations en entrée et réalisera une prédiction. Pour le réaliser, on pourra utiliser d'abord la régression logistique ```LogisticRegression``` de ```scikit-learn```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_representations(texts, vocabulary, embeddings, np_func=np.sum):\n",
    "    \"\"\"\n",
    "    Represent the sentences as a combination of the vector of its words.\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : a list of sentences   \n",
    "    vocabulary : dict\n",
    "        From words to indexes of vector.\n",
    "    embeddings : Matrix containing word representations\n",
    "    np_func : function (default: np.sum)\n",
    "        A numpy matrix operation that can be applied columnwise, \n",
    "        like `np.mean`, `np.sum`, or `np.prod`. \n",
    "    Returns\n",
    "    -------\n",
    "    np.array, dimension `(len(texts), embeddings.shape[1])`            \n",
    "    \"\"\"\n",
    "        \n",
    "    # Pour chaque texte 'text' du corpus 'texts', on considère l'embedding de chaque mot 'word' puis on applique\n",
    "    # la fonction 'np_func' sur cette liste d'embeddings, ce qui donne la représentation du texte courant.\n",
    "    \n",
    "    representations = np.zeros((len(texts), embeddings.shape[1]))\n",
    "    for k, text in enumerate(texts):\n",
    "        text_embedding = []\n",
    "        for word in clean_and_tokenize(text):\n",
    "            text_embedding.append(embeddings[vocabulary[unk_word(word, vocabulary)]])\n",
    "        representations[k]= np_func(text_embedding, axis=0)\n",
    "    return representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8384\n",
      "Score de classification Logistic Regression: 0.832 (std 0.0038)\n"
     ]
    }
   ],
   "source": [
    "## Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Exemple avec les embeddings obtenus via Gl0ve\n",
    "rep = sentence_representations(corpus, vocab_5k, GloveEmbeddings)\n",
    "clf = LogisticRegression(solver='lbfgs',max_iter=100000).fit(rep[::2], y[::2])\n",
    "print(clf.score(rep[1::2], y[1::2]))\n",
    "\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification Logistic Regression: %s (std %s)' % (round(np.mean(scores), 4), round(np.std(scores), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/.local/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/.local/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/pierre/.local/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/pierre/.local/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/pierre/.local/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de classification SVM: 0.8094 (std 0.0288)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/.local/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "## SVM\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Exemple avec les embeddings obtenus via Glove\n",
    "rep = sentence_representations(corpus, vocab_5k, GloveEmbeddings)\n",
    "# clf = LinearSVC(max_iter=25000).fit(rep[::2], y[::2])\n",
    "clf = LinearSVC(max_iter=100000).fit(rep[::2], y[::2])\n",
    "print(clf.score(rep[1::2], y[1::2]))\n",
    "\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification SVM: %s (std %s)' % (round(np.mean(scores), 4), round(np.std(scores), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez maintenant comparer l'ensemble des méthodes, et notamment répondre aux questions suivantes:\n",
    "- Pourquoi peut-on s'attendre à ce que les résultats obtenus avec les embeddings extraits des représentations pré-apprises avec Gl0ve soient bien meilleurs que les autres ? Quel serait le moyen de comparer de manière 'juste' Gl0ve avec les autres méthodes d'apprentissage de représentations ?\n",
    "- Quelle matrice permet d'obtenir les meilleures représentations via SVD ? (Co-occurences, Tf-Idf, PPMI ..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réponses\n",
    "\n",
    "On obtient les résultats suivants avec Gl0ve sur le corpus `texts` :\n",
    "\n",
    "|      Classifieur      | Score de classification croisée |\n",
    "|:---------------------:|:-------------------------------:|\n",
    "| Régression Logistique |       0.832 (std 0.004)         |\n",
    "| SVM (`max_iter=25000`)|       0.7918 (std 0.0658)       |\n",
    "|SVM (`max_iter=100000`)|       0.8094 (std 0.0288)       |\n",
    "\n",
    "- On constate donc effectivement que les résultats obtenus avec les _embeddings_ extraits des représentations pré-apprises avec Gl0ve sont bien meilleures que les résultats de la première partie. \n",
    "- On remarque par ailleurs que Gl0ve considère un vocabulaire de $400000$ mots avec une taille d'_embedding_ de $300$. Une façon plus « juste » de procéder pourrait consister à considérer une taille d'_embedding_ égale pour les autres méthodes de représentations, soit en la définissant de taille $300$, soit en réduisant celle de Gl0ve. Une telle façon de procéder permettrait de considérer des représentations de mots dans des espaces de même dimension et donc de comparer la qualité de l'essence même de ces _embeddings_.\n",
    "- On note énfin que les résultats obtenus pour la dernière partie considèrent le corpus `texts` et non plus `texts_reduced`, qui correspond à une fraction de $\\frac{1}{k} = 10 \\%$ (pour `k=10`) de l'ensemble de `texts`. Ainsi, dans la première partie, le jeu de données était bien moins complet. En utilisant Gl0ve avec le corpus réduit `texts_reduced`, les résultats obtenus seraient moins bons qu'ici.\n",
    "- La matrice permettant d'obtenir la meilleure représentationvia SVD semble être la PPMI. La matrice de co-occurences dont la dimension a été réduite présente également des résultats intéressants. D'une façon générale, les résultats obtenus avec TF-IDF sont moins convaincants."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
